<html>
<head>
<title>Tutorial Notes on Web-Scale Information Analytics</title>

<link href="/engg4030/styles/normalize.css" rel="stylesheet" />
<link href="/engg4030/styles/font-awesome.min.css" rel="stylesheet" />
<link rel="stylesheet" href="/engg4030/styles/tutorial.css" type="text/css" />

<script src="//code.jquery.com/jquery-1.11.0.min.js"></script>
<script src="//code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
<script src="/engg4030/scripts/polish.js"></script>

</head>
<body>


<div id="toc-holder" class="toc-holder">
    <div class="toc-button">
        <a href="#" class="toc-link" id="toc-link"><span>&#9660;</span> Table of Contents</a>
    </div>
    <ul id="toc" class="toc">
        
        <li class="toc-h2">
        <a class="toc-link" href="#system">System</a>
        </li>
        
        <li class="toc-h2">
        <a class="toc-link" href="#environment-and-dependencies">Environment and Dependencies</a>
        </li>
        
        <li class="toc-h3">
        <a class="toc-link" href="#java">Java</a>
        </li>
        
        <li class="toc-h3">
        <a class="toc-link" href="#hosts">Hosts</a>
        </li>
        
        <li class="toc-h3">
        <a class="toc-link" href="#password-less-ssh">Password-less SSH</a>
        </li>
        
        <li class="toc-h2">
        <a class="toc-link" href="#download-and-install-hadoop-package">Download and Install Hadoop Package</a>
        </li>
        
        <li class="toc-h2">
        <a class="toc-link" href="#configuration">Configuration</a>
        </li>
        
        <li class="toc-h3">
        <a class="toc-link" href="#environment-variables">Environment Variables</a>
        </li>
        
        <li class="toc-h3">
        <a class="toc-link" href="#hadoop-configurations">Hadoop Configurations</a>
        </li>
        
        <li class="toc-h2">
        <a class="toc-link" href="#test-hdfs">Test HDFS</a>
        </li>
        
        <li class="toc-h2">
        <a class="toc-link" href="#test-example-mapreduce-job">Test Example MapReduce Job</a>
        </li>
        
        <li class="toc-h2">
        <a class="toc-link" href="#more">More</a>
        </li>
        
        <li class="toc-h3">
        <a class="toc-link" href="#start-stop-the-cluster">Start/Stop the Cluster</a>
        </li>
        
        <li class="toc-h3">
        <a class="toc-link" href="#check-job-history">Check Job History</a>
        </li>
        
        <li class="toc-h2">
        <a class="toc-link" href="#references">References</a>
        </li>
        
        <li class="toc-h2">
        <a class="toc-link" href="#outcome-of-this-tutorial">Outcome of This Tutorial</a>
        </li>
        
    </ul>
</div><!-- .toc-holder -->

<header>
    <div class="sitetitle grey">
        <a href="/engg4030">Tutorial Notes on Web-Scale Information Analytics</a>
    </div>
    <h1 class="maintitle"> Single-Node Hadoop Setup </h1>
</header>



<article>
    <div id="content">
      <h1 id="single-node-hadoop-setup">Single Node Hadoop Setup</h1>
<p>Hadoop rolled out version 2 last year.
Since version 1 is still the mainstream,
we show how to setup hadoop-1.2.1 in this tutorial,
which is the latest version 1 release.</p>
<h2 id="system">System</h2>
<p>We work on the virtual machine created in last tutorial.
It runs Ubuntu 12.04 LTS distribution.
The following materials should apply to many other Linux distributions.
Just the specific commands may differ a bit.</p>
<h2 id="environment-and-dependencies">Environment and Dependencies</h2>
<h3 id="java">Java</h3>
<p>Hadoop requires Java Runtime Environment.
We install openjdk-6 for this tutorial.
See <a href="http://wiki.apache.org/hadoop/HadoopJavaVersions">the wiki</a>
for a list of compatible Java versions.</p>
<pre><code class="lang-bash">sudo apt-get update
sudo apt-get install openjdk-6-jdk
</code></pre>
<p>Verify if you have successfully installed Java6.</p>
<pre><code class="lang-bash">azureuser@test-hpl:~$ java -version
java version &quot;1.6.0_31&quot;
OpenJDK Runtime Environment (IcedTea6 1.13.3) (6b31-1.13.3-1ubuntu1~0.12.04.2)
OpenJDK 64-Bit Server VM (build 23.25-b01, mixed mode)
</code></pre>
<h3 id="hosts">Hosts</h3>
<p>Note, in order for Hadoop nodes to contact each other,
they need to resolve their own names.
The freshly launched VM on Azure (as of this writing) can not resolve its own name.
You can modify <code>/etc/hosts</code> such that it looks like the following:</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt/hadoop$ cat /etc/hosts
127.0.0.1 localhost

# The following lines are desirable for IPv6 capable hosts
::1 ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
ff02::3 ip6-allhosts

127.0.0.1 test-hpl
</code></pre>
<p>Note, change <code>test-hpl</code> to your own hostname,
which appears at the beginning of command line prompt <code>azureuser@test-hpl</code>.</p>
<h3 id="password-less-ssh">Password-less SSH</h3>
<p>First generate your key pairs:</p>
<pre><code class="lang-bash">azureuser@test-hpl:~$ cd .ssh
azureuser@test-hpl:~/.ssh$ ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/home/azureuser/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/azureuser/.ssh/id_rsa.
Your public key has been saved in /home/azureuser/.ssh/id_rsa.pub.
The key fingerprint is:
b2:5c:35:2f:04:7a:47:81:b1:23:5e:40:55:67:df:6e azureuser@test-hpl
The key&#39;s randomart image is:
+--[ RSA 2048]----+
|     .o.+++.o    |
|       o.+ o . . |
|      o = =   . .|
|     . + = o   . |
|      o S . .   E|
|     . +   .   . |
|      o          |
|                 |
|                 |
+-----------------+
</code></pre>
<p>Put the pubkey in the authorized key list:</p>
<pre><code class="lang-bash">azureuser@test-hpl:~/.ssh$ cat id_rsa.pub &gt;&gt; authorized_keys
</code></pre>
<p>Check the setup:</p>
<pre><code class="lang-bash">azureuser@test-hpl:~/.ssh$ ssh localhost
The authenticity of host &#39;localhost (127.0.0.1)&#39; can&#39;t be established.
ECDSA key fingerprint is 04:35:ad:f4:a1:cf:0d:c4:5e:c9:e4:65:6f:52:36:68.
Are you sure you want to continue connecting (yes/no)? yes
</code></pre>
<p>You will find that you re-loggged-in the same machine.
A new shell is allocated.
Remember to <code>exit</code> after the test.
The first time to SSH to a machine,
it will prompt you whether to accept the public key of the server.
After you typing <code>yes</code>,
the information will be recorded in <code>~/.ssh/known_hosts</code>.</p>
<p><strong>EXERCISE</strong>:
You can skip this section first and see what happens when you start the cluster.
Then you&#39;ll find the password-less SSH setup saves you some typing.
This is especially useful when you manage a big cluster.</p>
<p><strong>EXERCISE</strong>:
Explore the <code>~/.ssh/</code> folder.</p>
<h2 id="download-and-install-hadoop-package">Download and Install Hadoop Package</h2>
<p>We install Hadoop under <code>/opt/</code>.
First create the directories and give our user the ownership:</p>
<pre><code class="lang-bash">azureuser@test-hpl:~$ sudo mkdir -p /opt
azureuser@test-hpl:~$ sudo chown azureuser:azureuser /opt
azureuser@test-hpl:~$ cd /opt/
</code></pre>
<p>Verify we are in <code>/opt/</code> and have the read/write permissions.</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt$ pwd
/opt
azureuser@test-hpl:/opt$ ls -al .
total 8
drwxr-xr-x  2 azureuser azureuser 4096 Apr 28 07:05 .
drwxr-xr-x 23 root      root      4096 May  2 03:18 ..
</code></pre>
<p>Download Hadoop package from
<a href="http://archive.apache.org/dist/hadoop/core/">official repository</a>:</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt$ wget &#39;http://archive.apache.org/dist/hadoop/core/hadoop-1.2.1/hadoop-1.2.1.tar.gz&#39;
</code></pre>
<p>A good habit is to verify the downloaded package before installing it.
We first use <code>curl</code> to download the correct message digests for <code>hadoop-1.2.1.tar.gz</code>.
We cam then use <code>md5sum</code> and <code>sha1sum</code> to check the downloaded file.</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt$ curl http://archive.apache.org/dist/hadoop/core/hadoop-1.2.1/hadoop-1.2.1.tar.gz.mds
hadoop-1.2.1.tar.gz:    MD5 = 8D 79 04 80 56 17 C1 6C  B2 27 D1 CC BF E9 38 5A
hadoop-1.2.1.tar.gz:   SHA1 = B07B 88CA 658D C9D3 38AA  84F5 C68C 809E B7C7 0964
hadoop-1.2.1.tar.gz: RMD160 = 6330 DED6 043A 1C8D D859  7910 E77F 3DED F249 A807
hadoop-1.2.1.tar.gz: SHA224 = 3500FE1F 513A32D7 AD3EEBA1 F177710C 3D678534
                              CD6DA4F4 13C8188E
hadoop-1.2.1.tar.gz: SHA256 = 94A11817 71F173BD B55C8F90 17228258 66396091
                              F0516BDD 12B34DC3 DE1706A1
hadoop-1.2.1.tar.gz: SHA384 = 2ABAF8DB 781FB3EA 11621937 1847445C 44B5C7D7
                              48EC8410 43D96C9A 9D6DD978 F300CE18 F02D9FC8
                              ED6B8176 D08E5B62
hadoop-1.2.1.tar.gz: SHA512 = 79C6423D 1E0E2835 98442DCF FD63DA52 BF3AB53B
                              80957243 7BAF8DA8 38B592E1 E776430E A67DB53E
                              52B78112 5BCAB225 DC222632 63CDF185 7D2A7A46
                              A4966DA8
azureuser@test-hpl:/opt$ md5sum hadoop-1.2.1.tar.gz
8d7904805617c16cb227d1ccbfe9385a  hadoop-1.2.1.tar.gz
azureuser@test-hpl:/opt$ sha1sum hadoop-1.2.1.tar.gz
b07b88ca658dc9d338aa84f5c68c809eb7c70964  hadoop-1.2.1.tar.gz
</code></pre>
<p>Uncompress the downloaded <code>.tar.gz</code> archive.
Instead of operating on directory <code>hadoop-1.2.1</code>,
it is better to soft link it to <code>hadoop</code>.
In this way,
you don&#39;t have to modify other programs
when you upgrade your Hadoop version.</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt$ tar -xzvf hadoop-1.2.1.tar.gz
...
azureuser@test-hpl:/opt$ ln -s hadoop-1.2.1 hadoop
azureuser@test-hpl:/opt$ ls
hadoop  hadoop-1.2.1  hadoop-1.2.1.tar.gz
</code></pre>
<h2 id="configuration">Configuration</h2>
<h3 id="environment-variables">Environment Variables</h3>
<p>Export the environment variables in your <code>~/.bashrc</code>.
You can use <code>vim</code> to edit the file
or use <code>cat &gt;&gt; ~/.bashrc</code> followed by an input stream.
Check your configuration:</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt/hadoop/conf$ tail ~/.bashrc
# sources /etc/bash.bashrc).
if [ -f /etc/bash_completion ] &amp;&amp; ! shopt -oq posix; then
    . /etc/bash_completion
fi

export HADOOP_PREFIX=/opt/hadoop
export HADOOP_HOME=$HADOOP_PREFIX
export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64
export PATH=$PATH:$HADOOP_HOME/bin
</code></pre>
<p>You can activate those environment variables by <code>source ~/.bashrc</code>.
This configuration will also be loaded every time you login.</p>
<p>Now issue the <code>hadoop</code> command.
You should be able to see the following prompt.</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt/hadoop/conf$ hadoop
Warning: $HADOOP_HOME is deprecated.

Usage: hadoop [--config confdir] COMMAND
where COMMAND is one of:
  namenode -format     format the DFS filesystem
  secondarynamenode    run the DFS secondary namenode
  namenode             run the DFS namenode
  datanode             run a DFS datanode
...
</code></pre>
<p><strong>TIP</strong>:
You may see a warning &quot;Warning: $HADOOP_HOME is deprecated.&quot;.
The reason is that <code>HADOOP_HOME</code> is deprecated.
You can use <code>HADOOP_PREFIX</code> as the new recommended environment variable.
(<i class="fa fa-thumbs-up fa-fw"></i> Robin Lee)</p>
<p><strong>TIP</strong>:
You may see a warning that deprecates <code>localhost:9000</code> for <code>fs.default.name</code>.
Use <code>hdfs://localhost:9000</code> to solve it.
(<i class="fa fa-thumbs-up fa-fw"></i> Gao Ruohan)</p>
<h3 id="hadoop-configurations">Hadoop Configurations</h3>
<p>Hadoop configuration files are in <code>/opt/hadoop/conf</code>:</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt/hadoop/conf$ ls
capacity-scheduler.xml  hadoop-metrics2.properties  mapred-site.xml         taskcontroller.cfg
configuration.xsl       hadoop-policy.xml           masters                 task-log4j.properties
core-site.xml           hdfs-site.xml               slaves
fair-scheduler.xml      log4j.properties            ssl-client.xml.example
hadoop-env.sh           mapred-queue-acls.xml       ssl-server.xml.example
</code></pre>
<p>Modify the configuration as follows.
The output is <a href="http://git-scm.com/docs/git-diff">Git Diff</a> format.
A line starting with <code>-</code> means we removed it.
A line starting with <code>+</code> means we added it.
Before the content diff is presented, there will be two lines showing the file name,
e.g. <code>core-site.xml</code>.</p>
<pre><code class="lang-diff">index 970c8fe..317d9ba 100644
--- a/core-site.xml
+++ b/core-site.xml
@@ -5,4 +5,13 @@

 &lt;configuration&gt;

+  &lt;property&gt;
+    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
+    &lt;value&gt;/opt/hadoop-tmp&lt;/value&gt;
+    &lt;description&gt;A base for other temporary directories.&lt;/description&gt;
+  &lt;/property&gt;
+  &lt;property&gt;
+     &lt;name&gt;fs.default.name&lt;/name&gt;
+     &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
+  &lt;/property&gt;
 &lt;/configuration&gt;
diff --git a/hadoop-env.sh b/hadoop-env.sh
index 01654b9..97ccb79 100644
--- a/hadoop-env.sh
+++ b/hadoop-env.sh
@@ -6,7 +6,7 @@
 # remote nodes.

 # The java implementation to use.  Required.
-# export JAVA_HOME=/usr/lib/j2sdk1.5-sun
+export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64

 # Extra Java CLASSPATH elements.  Optional.
 # export HADOOP_CLASSPATH=
diff --git a/hdfs-site.xml b/hdfs-site.xml
index 970c8fe..4e52fa4 100644
--- a/hdfs-site.xml
+++ b/hdfs-site.xml
@@ -5,4 +5,8 @@

 &lt;configuration&gt;

+     &lt;property&gt;
+         &lt;name&gt;dfs.replication&lt;/name&gt;
+         &lt;value&gt;1&lt;/value&gt;
+     &lt;/property&gt;
 &lt;/configuration&gt;
diff --git a/mapred-site.xml b/mapred-site.xml
index 970c8fe..5d8b379 100644
--- a/mapred-site.xml
+++ b/mapred-site.xml
@@ -5,4 +5,9 @@

 &lt;configuration&gt;

+     &lt;property&gt;
+         &lt;name&gt;mapred.job.tracker&lt;/name&gt;
+         &lt;value&gt;localhost:9001&lt;/value&gt;
+     &lt;/property&gt;
 &lt;/configuration&gt;
+
</code></pre>
<p><strong>TIP</strong>:
Use ctrl+d to end the input when you use the <code>cat &gt;</code> approach to create file.</p>
<p><strong>TIP</strong>:
A crash course of VIM:
start VIM by <code>vim file-to-edit</code>;
use <code>i</code> to enter the insert mode;
move cursor by arrow keys;
do your edits;
type <code>&lt;ESC&gt;</code> to end insert mode;
type <code>:wq</code> to save and quit the editor.
You are suggested to learn more about VIM after the tutorial.</p>
<h2 id="test-hdfs">Test HDFS</h2>
<p>Before you start the cluster for the first time,
you need to prepare some data structure for HDFS&#39;s namenode.
This is done via following <code>-format</code> command.
You can also check <code>/opt/hadoop-tmp/</code> to see some directories and files are created.</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt/hadoop$ hadoop namenode -format
...
azureuser@test-hpl:/opt/hadoop$ ls /opt/hadoop-tmp/
dfs
</code></pre>
<p>Once ready, you can start HDFS using <code>start-dfs.sh</code> script.
This by default launches a single node cluster.
Check whether <code>NameNode</code>, <code>SecondaryNameNode</code> and <code>DataNode</code> are running.</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt/hadoop$ start-dfs.sh
...
azureuser@test-hpl:/opt/hadoop$ jps
12549 NameNode
12939 SecondaryNameNode
12741 DataNode
13012 Jps
</code></pre>
<p>If you know the <code>ps</code> command for Linux,
<code>jps</code> is an analogy of <code>ps</code> for Java processes.
Although you can use system&#39;s <code>ps</code> to check Java process,
e.g. `ps aux | grep java,
The output is too long to comprehend.</p>
<p>If the cluster is running,
you can operate HDFS using command <code>hadoop dfs XXXX</code>.
Type <code>hadoop dfs</code> to see a list of commands.
The names are very like other linux commands, e.g. <code>-ls</code>, <code>-mv</code>, ...</p>
<p>Test create/delete a dir under the root:</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt/hadoop$ hadoop dfs -ls /
azureuser@test-hpl:/opt/hadoop$ hadoop dfs -mkdir /testdir
azureuser@test-hpl:/opt/hadoop$ hadoop dfs -ls /
Found 1 items
drwxr-xr-x   - azureuser supergroup          0 2014-05-02 04:42 /testdir
azureuser@test-hpl:/opt/hadoop$ hadoop dfs -rmr /testdir
Deleted hdfs://localhost:9000/testdir
azureuser@test-hpl:/opt/hadoop$ hadoop dfs -ls /
</code></pre>
<p>Upload a test file:</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt/hadoop$ hadoop dfs -ls /
azureuser@test-hpl:/opt/hadoop$ hadoop dfs -copyFromLocal README.txt /README.txt
azureuser@test-hpl:/opt/hadoop$ hadoop dfs -ls /
azureuser@test-hpl:/opt/hadoop$ hadoop dfs -tail /README.txt
try, of
encryption software.  BEFORE using any encryption software, please
check your country&#39;s laws, regulations and policies concerning the
import, possession, or use, and re-export of encryption software, to
see if this is permitted.  See &lt;http://www.wassenaar.org/&gt; for more
information.
...
</code></pre>
<p><strong>EXERCISE</strong>:
Checkout <code>-copyToLocal</code> to download a file.</p>
<p>You can use <code>stop-dfs.sh</code> to stop the HDFS cluster.
Let’s keep running for now because the Hadoop MapReduce runs on HDFS.</p>
<p><strong>NOTE</strong>:
You need to work step by step.
If some components are not running, e.g. <code>DataNode</code>,
you can try to check the logs usually located at <code>$HADOOP_PREFIX/logs</code>.</p>
<h2 id="test-example-mapreduce-job">Test Example MapReduce Job</h2>
<p>Start the Hadoop MapReduce and check running processes by <code>jps</code>.
There are two more processes: <code>JobTracker</code> and <code>TaskTracker</code>.</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt/hadoop$ start-mapred.sh
starting jobtracker, logging to /opt/hadoop-1.2.1/libexec/../logs/hadoop-azureuser-jobtracker-test-hpl.out
localhost: starting tasktracker, logging to /opt/hadoop-1.2.1/libexec/../logs/hadoop-azureuser-tasktracker-test-hpl.out
azureuser@test-hpl:/opt/hadoop$ jps
12549 NameNode
12939 SecondaryNameNode
13834 Jps
12741 DataNode
13600 JobTracker
13785 TaskTracker
</code></pre>
<p>The there is an example suite in hadoop-1.2.1 package.
Find the list of examples as follows:</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt/hadoop$ hadoop jar hadoop-examples-1.2.1.jar
An example program must be given as the first argument.
Valid program names are:
  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.
  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.
  dbcount: An example job that count the pageview counts from a database.
  grep: A map/reduce program that counts the matches of a regex in the input.
  join: A job that effects a join over sorted, equally partitioned datasets
  multifilewc: A job that counts words from several files.
  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.
  pi: A map/reduce program that estimates Pi using monte-carlo method.
  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.
  randomwriter: A map/reduce program that writes 10GB of random data per node.
  secondarysort: An example defining a secondary sort to the reduce.
  sleep: A job that sleeps at each map and reduce task.
  sort: A map/reduce program that sorts the data written by the random writer.
  sudoku: A sudoku solver.
  teragen: Generate data for the terasort
  terasort: Run the terasort
  teravalidate: Checking results of terasort
  wordcount: A map/reduce program that counts the words in the input files.
</code></pre>
<p>We try the wordcount example.
We need to know how to pass parameters:</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt/hadoop$ hadoop jar hadoop-examples-1.2.1.jar  wordcount
Usage: wordcount &lt;in&gt; &lt;out&gt;
</code></pre>
<p>Then we do a wordcount using the <code>README.txt</code> we uploaded to HDFS in last section.</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt/hadoop$ hadoop jar hadoop-examples-1.2.1.jar  wordcount /README.txt /output/
14/05/02 05:25:11 INFO input.FileInputFormat: Total input paths to process : 1
14/05/02 05:25:11 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/05/02 05:25:11 WARN snappy.LoadSnappy: Snappy native library not loaded
14/05/02 05:25:12 INFO mapred.JobClient: Running job: job_201405020524_0001
14/05/02 05:25:13 INFO mapred.JobClient:  map 0% reduce 0%
14/05/02 05:25:21 INFO mapred.JobClient:  map 100% reduce 0%
14/05/02 05:25:30 INFO mapred.JobClient:  map 100% reduce 33%
14/05/02 05:25:31 INFO mapred.JobClient:  map 100% reduce 100%
14/05/02 05:25:33 INFO mapred.JobClient: Job complete: job_201405020524_0001
...
</code></pre>
<p>Check the output after the job is finished:</p>
<pre><code class="lang-bash">azureuser@test-hpl:/opt/hadoop$ hadoop dfs -ls /
Found 3 items
-rw-r--r--   1 azureuser supergroup       1366 2014-05-02 04:54 /README.txt
drwxr-xr-x   - azureuser supergroup          0 2014-05-02 05:24 /opt
drwxr-xr-x   - azureuser supergroup          0 2014-05-02 05:25 /output
azureuser@test-hpl:/opt/hadoop$ hadoop dfs -ls /output/
Found 3 items
-rw-r--r--   1 azureuser supergroup          0 2014-05-02 05:25 /output/_SUCCESS
drwxr-xr-x   - azureuser supergroup          0 2014-05-02 05:25 /output/_logs
-rw-r--r--   1 azureuser supergroup       1306 2014-05-02 05:25 /output/part-r-00000
azureuser@test-hpl:/opt/hadoop$ hadoop dfs -tail /output/part-r-00000
ty    1
License    1
Number    1
Regulations,    1
SSL    1
Section    1
Security    1
See    1
Software    2
</code></pre>
<h2 id="more">More</h2>
<h3 id="start-stop-the-cluster">Start/Stop the Cluster</h3>
<p>You can use <code>start-all.sh</code> and <code>stop-all.sh</code>
to start and stop the entire cluster,
including HDFS and MapReduce.</p>
<h3 id="check-job-history">Check Job History</h3>
<pre><code class="lang-bash">azureuser@test-hpl:/opt/hadoop$ hadoop job -history /output/

Hadoop job: 0001_1399008311838_azureuser
=====================================
Job tracker host name: job
job tracker start time: Thu May 20 01:50:20 UTC 1976
</code></pre>
<h2 id="references">References</h2>
<ul>
<li>Hadoop streaming:
<a href="http://hadoop.apache.org/docs/stable1/streaming.html">http://hadoop.apache.org/docs/stable1/streaming.html</a></li>
<li>Generic command options:
<a href="http://hadoop.apache.org/docs/stable1/streaming.html#Generic+Command+Options">http://hadoop.apache.org/docs/stable1/streaming.html#Generic+Command+Options</a></li>
<li>Streaming commmand options:
<a href="http://hadoop.apache.org/docs/stable1/streaming.html#Streaming+Command+Options">http://hadoop.apache.org/docs/stable1/streaming.html#Streaming+Command+Options</a></li>
</ul>
<h2 id="outcome-of-this-tutorial">Outcome of This Tutorial</h2>
<ul>
<li>Have a basic idea of Hadoop package.</li>
<li>Have a basic idea of the workflow of running a Hadoop MapReduce job.</li>
</ul>

    </div>

    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'engg4030'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</article>

    <div id="shortcut">
        <a href="/engg4030">&#9654; Back</a>
        <a href="#top">&#9650; Top</a>
    </div>

    

<footer>

<a rel="license" href="http://creativecommons.org/licenses/by/4.0/">
<img alt="Creative Commons License" style="" src="http://i.creativecommons.org/l/by/4.0/88x31.png" />
</a>
All tutorials including supplementary materials, e.g. IPython Notebooks, are licensed under 
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/">
Creative Commons Attribution 4.0 International License</a>.

<br>

Content and page design by Pili Hu.
Contact:
<a href="https://github.com/hupili">
<i class="fa fa-fw fa-github"></i>
</a>
<a href="https://twitter.com/hupili">
<i class="fa fa-fw fa-twitter"></i>
</a>
<a href="http://weibo.com/impige">
<i class="fa fa-fw fa-weibo"></i>
</a>
<a href="https://facebook.com/hupili">
<i class="fa fa-fw fa-facebook"></i>
</a>

<br>

Source on GitHub: 
<a href="https://github.com/hupili/engg4030">https://github.com/hupili/engg4030</a>

</footer>

<script src="/engg4030/scripts/jquery.fixedTOC.js"></script>
<script>
// call the plugin on the "#toc" element
$(function(){
    $('#toc').fixedTOC({
        menuOpens: 'click',
        scrollSpeed: 1000,
        menuSpeed: 300,
        useSubMenus: false,
        resetSubMenus: false,
        topLinkWorks: true
    });
});
</script>



<link rel="stylesheet" href="/engg4030/styles/highlightjs-default.css">
<script src="/engg4030/scripts/highlight.pack.js"></script>
<script>
hljs.initHighlightingOnLoad();
</script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-37311363-9', 'hupili.net');
  ga('send', 'pageview');
</script>

</body>
</html>
